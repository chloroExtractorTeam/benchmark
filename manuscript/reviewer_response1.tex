\documentclass[a4paper,10pt]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\title{Response to Reviewers}
\author{Markus J. Ankenbrand and Frank F\"{o}rster for the authors}
\date{\today}
%
\begin{document}
\maketitle
\section{Editor}
We have now received reports from two referees, whose comments are available below.
As you will see from the reports, both referees raised serious concerns that the benchmark hasn't provided sufficient advance to be useful for the field.
At this stage, therefore, we are not persuaded that an offer of publication in Genome Biology would be justified.
Unless you can address all of the issues raised by the referees I am afraid that we would not be able to consider this work again.
In particular we ask that you provide additional analyses as requested by both referees to convince them of the advance.
The revised manuscript should address in full all the points raised by both referees; a separate list of the revisions made, specifying where in the manuscript the changes have been made, would also be helpful.
Please note that all data generated for the manuscript should be deposited in a relevant public repository, with accession codes listed in the manuscript's Availability of Data and Materials section.
All other supporting data should also be signposted.
If you decide to submit a revised version, you will need to  this as a new manuscript, although we would consider it in the same way as we do all revised manuscripts.
The exception is that we will take into account any work published in the intervening period.
For the time being, however, we hope that you will find our referees' comments helpful in deciding how to proceed.
Please feel free to contact us if you have any questions.

\section{Reviewer 1}
Methods are appropriate but same authors developed ChloroExtractor, an important tool used as a benchmark in this study.
Authors compare seven chloroplast genome assembly tools.
So, this reviewer is not sure of novelty of this manuscript.
So, authors haven't addressed the important question "Does the work represent a significant advance over previously published studies?".  

\textbf{
Indeed the novelty in this article is not a new approach to the task of chloroplast genome assembly but rather a thourough benchmark of existing tools (including our tool ChloroExtractor).
This represents a substantial advance for the field as there are competing tools published for the same task without any information about their relative performances.
Currently, researchers have to do this evaluation of tools themselves to find the appropriate tool which leads to duplicated efforts as this is done by every group separately and without sharing results.
If this evaluation is neglected a suboptimal tool might be used which can lead to additional manual efforts in finishing or additional costs if more data is generated.
Thus, clear guidelines to the end user (and important directions for improvement to the developers) are novel, important and benficial for the research community.
}

User experience and success rate should be determined by users and documented through satisfaction surveys.
This is especially important when authors emphasize that program can be used using standard computer infrastructure.

\textbf{
We agree that this would be the best way to measure and quantify user experience.
However, this is not feasible with limited budget and time (TODO: is this excuse good enough?) while the main focus of the work was on quantitative performance measures, anyway.
We decided to include usability metrics that were obtainable in a defined and structured way using the JOSS reviewer guidelines.
}

There is >1,000 sequenced chloroplast genomes, with a large majority not fully assembled or annotated.
So, authors have plenty of opportunity to demonstrate use of these new assembly tools.

\textbf{
Thanks for pointing this out.
We now include as a proof of principle the application of these tools on more than 100 novel datasets.
We thus clearly demonstrate that the insights from this article can be used to produce further novel results that can lead to biological insight.
}

It is well known that intergenic spacer regions are not conserved and not even a single intergenic spacer region is conserved within chloroplasts genomes of the graminae or grass genomes.
Intergenic spacer region occupies >50\% of the genome.
So, how does reference genome help in genome assembly?

\textbf{
This is a very interesting question.
We don't think that this is within the scope of our publication as we compare de novo assembly tools.
TODO: some people use it, some programs use it, it seems to work often but actually your argument is justification why we restricted our analysis to de novo tools.
}

In order to have real life comparison, authors could have sequenced few chloroplast genomes and used that data to compare different assembly tools and time taken for each program, pros and cons.

\textbf{
We already tested the tools on real datasets downloaded from SRA which is equivalent to customly sequenced data.
But we now include additional 100+ novel data sets with no reference chloroplast in the database.
}

- Is the paper of broad interest to others in the field, or of outstanding interest to a broad audience of biologists?
This is of interest to evolutionary biologists to study phylogenetics or for chloroplast genetic engineering.

\section{Reviewer 2}
This study provides an systematically comparison of the chloroplast assembly tools at first and show significant differences between the tested assemblers in terms of generating whole chloroplast genome sequences and computational requirements.
Moreover, the authors created docker images for each tested tool, which are available for the scientific community and improved reproducibility, thus large scale screening for chloroplasts as hidden treasures within genomic sequencing data is feasible.
This topic of the review paper is knowledgeable and helpful for all the scientists who are working in the field of plant chloroplast assembly.
However, the overall manuscript is not well written and contains limited information about systematically comparison of the chloroplast assembly tools and docker images for assembled reproducibility.

\textbf{
Thanks for your thorough review and the pointers that helped us improve the overall readability of the manuscript.
In addition we received feedback and edits from a native English speaker to further improve readabilty.
TODO: how do we adress the "limited information" aspect. We could link to the github repo that contains all data and code we used including the dockerfiles.
}

Specific points: The number of lines on the edge does not correspond to the maintext.

The major concerns about this manuscript can be found below and major revision is suggested before publication in "Genome Biology".

\begin{itemize}
    \item Page1  part of "General introduction and motivation"
    \item Line 4: It should be quote recent article
    \item Line 11: It should be quote recent article
    \item Page2
    \item Line 16: It should be IRa and IRb
    \item Line 18: It should be quote recent article
    \item Line 24-27: Re-write sentence
    \item Part of "Approaches to extracting chloroplasts from whole genome data"
    \item Line 6: Delete the dot before the quote
    \item Lines 7-9: Re-write sentence
    \item Part of "Purpose and scope of this study"
    \item Line 2: It should be in italic type
    \item Line 2: It should be genome
    \item Line 3: Raw genomic data sets should be filter out low-quality reads by quality control
    \item Lines 4 and 5: Many command-line tools of de novo assemble for chloroplast genome have all of these features, e.g., MITObim.
    \item Part of "Time requirements"
    \item Line 1: It should be among
    \item Line 4: Different tools with more than 1 gigabyte data sets should be tested, and Figure1 can`t give valuable information as no significant difference for time-consuming. The same question for Part of "Memory and CPU Usage". Why these seven assemblers were chosen to test? Figure should be "Fig." and table as "Table".
    \item Part of "Qualitative"
    \item Line 3: NGS data for crop genomes can`t be used for GetOrganelle? In this part, the same data set should be used to test for every assemblers better.
    \item Part of "Quantitative"  
    \item Part of "Simulated data"
    \item Line 1: Why not show the result for different data sets with every assemblers in Fig.3 and Table 2? Fig.3 and table 2 show no regular pattern in scores except the assembler GetOrganelle obtaining perfect.
    \item Part of "Real data sets"
    \item Line 3: The performance of GetOrganelle was bad, as only 199 circular assemblies out of a total of 356 assemblies that resulted in an output, the percent is 55.9\%, however it achieved a median scores of 99.7. The number "99.7" should to be "89.13", as show in table 3? The SD value of GetOrganelle is too large than the other three tools but it achieved the highest scores.
    \item Part of "Consistency"
    \item Line 3: As shown in the Fig.6, only two tools achieved R2  not equal 1 by re-running assemblies and comparison of the scores of two assemblers, however, GetOrganelle defined as the only tool that succeed in obtaining similar score for all assemblers, this conclusion is not rigorous.
\end{itemize}
\end{document}